{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3496df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ColQWEN model\n",
    "import torch\n",
    "from colpali_engine.models import ColQwen2, ColQwen2Processor\n",
    "import os\n",
    "\n",
    "# Get rid of process forking deadlock warnings.\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda:0\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "# A convenience class to wrap the functionality we will use from\n",
    "# https://huggingface.co/vidore/colqwen2-v1.0\n",
    "class Colqwen:\n",
    "    def __init__(self):\n",
    "        \"\"\"Load the model and processor from huggingface.\"\"\"\n",
    "        # About a 5 GB download and similar memory usage.\n",
    "        self.model = ColQwen2.from_pretrained(\n",
    "            \"vidore/colqwen2-v1.0\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=get_device(),  # or \"cuda:0\" if using a NVIDIA GPU\n",
    "            attn_implementation=\"eager\",  # or \"flash_attention_2\" if available\n",
    "        ).eval()\n",
    "        self.processor = ColQwen2Processor.from_pretrained(\"vidore/colqwen2-v1.0\")\n",
    "\n",
    "    # A batch size of one appears to be most performant when running on an M4.\n",
    "    # Note: Reducing the image resolution speeds up the vectorizer and produces\n",
    "    # fewer multi-vectors.\n",
    "    def multi_vectorize_image(self, img):\n",
    "        \"\"\"Return the multi-vector image of the supplied PIL image.\"\"\"\n",
    "        image_batch = self.processor.process_images([img]).to(self.model.device)\n",
    "        with torch.no_grad():\n",
    "            image_embedding = self.model(**image_batch)\n",
    "        return image_embedding[0]\n",
    "\n",
    "    def multi_vectorize_text(self, query):\n",
    "        \"\"\"Return the multi-vector embedding of the query text string.\"\"\"\n",
    "        query_batch = self.processor.process_queries([query]).to(self.model.device)\n",
    "        with torch.no_grad():\n",
    "            query_embedding = self.model(**query_batch)\n",
    "        return query_embedding[0]\n",
    "\n",
    "    def maxsim(self, query_embedding, image_embedding):\n",
    "        \"\"\"Compute the MaxSim between the query and image multi-vectors.\"\"\"\n",
    "        return self.processor.score_multi_vector(\n",
    "            [query_embedding], [image_embedding]\n",
    "        ).item()\n",
    "\n",
    "\n",
    "# Instantiate the model to be used below.\n",
    "colqwen = Colqwen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "def get_embedding(image_path):\n",
    "    \"\"\"Return a generated multi vector embedding for a given image\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    embedding = colqwen.multi_vectorize_image(image)\n",
    "    return embedding.detach().cpu().to(dtype=torch.float32).numpy()\n",
    "\n",
    "def generate_embeddings():\n",
    "    \"\"\"Generate multi vector embeddings for all PDF pages\"\"\"\n",
    "    pages_folder = \"./pdf/pages\"\n",
    "    png_files = [f for f in os.listdir(pages_folder) if f.endswith('.png')]\n",
    "    embeddings = []\n",
    "    for file_name in png_files:\n",
    "        emb = get_embedding(f\"{pages_folder}/{file_name}\")\n",
    "        embeddings.append((file_name, emb))\n",
    "    return embeddings\n",
    "\n",
    "def generate_or_load_cached_embeddings(force_generate = False):\n",
    "    \"\"\"Get the cached multi vector embeddings or generate them if they are not present\"\"\"\n",
    "    try:\n",
    "        cached_embeddings = \"./pdf/multi_vector_embeddings.json\"\n",
    "        if os.path.exists(cached_embeddings) and not force_generate:\n",
    "            data = []\n",
    "            with open(cached_embeddings, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            return data\n",
    "        else:\n",
    "            embeddings = generate_embeddings()\n",
    "            data = []\n",
    "            for emb in embeddings:\n",
    "                data.append({\n",
    "                    \"paper_number\": emb[0],\n",
    "                    \"embedding\": emb[1].tolist()  # Convert NumPy array to list for JSON\n",
    "                })\n",
    "            with open(cached_embeddings, \"w\") as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be16f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Weaviate v1.31.0 in the background using docker\n",
    "!docker run --detach -p 8080:8080 -p 50051:50051 cr.weaviate.io/semitechnologies/weaviate:1.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1720589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.config import Property, DataType, Configure\n",
    "from weaviate.util import generate_uuid5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f49832",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.connect_to_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de735c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.collections.delete(\"NvidiaPDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia_pdf = client.collections.create(\n",
    "        name=\"NvidiaPDF\",\n",
    "        properties=[\n",
    "            Property(name=\"paper_number\", data_type=DataType.TEXT),\n",
    "        ],\n",
    "        vectorizer_config=[Configure.NamedVectors.none(\n",
    "            name=\"colqwen\",\n",
    "            vector_index_config=Configure.VectorIndex.hnsw(\n",
    "                multi_vector=Configure.VectorIndex.MultiVector.multi_vector(\n",
    "                    encoding=Configure.VectorIndex.MultiVector.Encoding.muvera(),\n",
    "                ),\n",
    "            )\n",
    "        )]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4973f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = generate_or_load_cached_embeddings(force_generate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d502c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for emb in embeddings:\n",
    "    print(f\"page_number: {emb[\"paper_number\"]} embedding: {len(emb[\"embedding\"])}\")\n",
    "print(f\"Total: {len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8a7a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.collections.get(\"NvidiaPDF\")\n",
    "with collection.batch.dynamic() as batch:\n",
    "    for emb in embeddings:\n",
    "        batch.add_object(\n",
    "            uuid=generate_uuid5(emb[\"paper_number\"]),\n",
    "            properties={\"paper_number\": emb[\"paper_number\"]}, vector={\"colqwen\": emb[\"embedding\"]},\n",
    "        )\n",
    "    batch.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8969657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def get_query_embedding(query: str):\n",
    "    \"\"\"Generates multi vector embedding for a query string\"\"\"\n",
    "    query_emb = colqwen.multi_vectorize_text(query)\n",
    "    return query_emb.cpu().to(dtype=torch.float32).numpy()\n",
    "\n",
    "def perform_query_and_show_first_result(query):\n",
    "    \"\"\"A helper method which performs a vector search an shows first result\"\"\"\n",
    "    res = collection.query.near_vector(\n",
    "        near_vector=get_query_embedding(query),\n",
    "        limit=1\n",
    "    )\n",
    "    def show_image(file_name):\n",
    "        pages_folder = \"./pdf/pages\"\n",
    "        image = Image.open(f\"{pages_folder}/{file_name}\")\n",
    "        image.show()\n",
    "    show_image(res.objects[0].properties[\"paper_number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_query_and_show_first_result(\"List of countries using AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e69cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_query_and_show_first_result(\"What is Nvidia's infrastructure roadmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55a8f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_query_and_show_first_result(\"revenue and income charts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1fa440",
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_query_and_show_first_result(\"NVIDIA CUDA speedup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6e4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_query_and_show_first_result(\"What are the plans for dividends?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
